{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='/home/gops/projects/woodfrog.tech/spellchecker/data/corpus.txt'\n",
    "try:\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "except FileNotFoundError:\n",
    "    print(f\"File '{file_path}' not found.\")\n",
    "except IOError:\n",
    "    print(f\"Error reading file '{file_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12802235"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gops/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/gops/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1196734"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokens = word_tokenize(content)\n",
    "\n",
    "# Cleaning\n",
    "cleaned_tokens = [token for token in tokens if token.isalpha()]\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "#\n",
    "filtered_tokens = [token for token in cleaned_tokens if token not in stopwords]\n",
    "\n",
    "# Display the extracted words\n",
    "len(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1196734/1196734 [00:08<00:00, 134098.44it/s]\n"
     ]
    }
   ],
   "source": [
    "import enchant\n",
    "ench_d = enchant.Dict(\"en_US\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "misspelled =[]\n",
    "for word in tqdm(filtered_tokens):\n",
    "    if ench_d.check(word) == False:\n",
    "        misspelled.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17475"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing Duplicats \n",
    "misspelled_words=list(set(misspelled))\n",
    "len(set(misspelled_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grfts',\n",
       " 'sapraemia',\n",
       " 'mnage',\n",
       " 'Loded',\n",
       " 'Araham',\n",
       " 'Razumovski',\n",
       " 'evastated',\n",
       " 'onations',\n",
       " 'Neihardt',\n",
       " 'camlet']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misspelled_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nations',\n",
       " 'donations',\n",
       " 'orations',\n",
       " 'ovations',\n",
       " 'o nations',\n",
       " 'nationals',\n",
       " 'intonation',\n",
       " 'donation',\n",
       " 'citations',\n",
       " 'unctions',\n",
       " 'insinuation']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word='onations'\n",
    "corrected_word = ench_d.suggest(word)\n",
    "corrected_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1={word:corrected_word}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-5f2a12d4daee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdict1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'append'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 20.33it/s]\n"
     ]
    }
   ],
   "source": [
    "file3 = open('enchant_test.txt', 'w')\n",
    "import json \n",
    "json_op=[]\n",
    "for word in tqdm(misspelled_words[0:10]):\n",
    "    corrected_word = ench_d.suggest(word)\n",
    "    dict_t={word:corrected_word}\n",
    "    json_op.append(dict_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.json', 'w') as file:\n",
    "    json.dump(json_op, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'grfts': ['grafts', 'grits', 'gifts', 'grists', 'gravitas', 'graft']}, {'sapraemia': ['Semiramis', 'misapplier', 'supremacy', 'supremal']}, {'mnage': ['mange', 'menage', 'manage', 'nonage', 'nagged', 'nagger', 'Germana', 'munge', 'manège']}, {'Loded': ['Oded', 'Roded', 'Coded', 'Boded', 'L oded', 'Doled', 'Lodes', 'Lode', 'Loaded', 'Lorded', 'Lodged', 'Laded', 'Loped', 'Lobed', 'Loved']}, {'Araham': ['Abraham', 'Graham', 'Ara ham', 'Ara-ham', 'Tamarah', 'Maharajah', 'Markham', 'Amara', 'Ibrahim', 'Arrowhead']}, {'Razumovski': ['Phrasemaking', 'Radiophysics']}, {'evastated': ['devastated', 'devastate', 'devastator', 'devastating', 'devastation', 'estated', 'overstate', 'Hofstadter']}, {'onations': ['nations', 'donations', 'orations', 'ovations', 'o nations', 'nationals', 'intonation', 'donation', 'citations', 'unctions', 'insinuation']}, {'Neihardt': ['Reinhardt', 'Bernhardt', 'Reinhard', 'Gerhardt', 'Rinehart', 'Openhearted']}, {'camlet': ['caplet', 'hamlet', 'Hamlet', 'cam let', 'cam-let', 'Camile', 'Camille', 'Capulet', 'armlet', 'Camelot', 'Carmelita']}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('data.json', 'r') as file:\n",
    "    json_data = file.read()\n",
    "    data = json.loads(json_data)\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Step 1: Open the JSON file\n",
    "with open('data.json', 'r') as file:\n",
    "    # Step 2: Read the contents of the file\n",
    "    json_data = file.read()\n",
    "\n",
    "    # Step 3: Parse the JSON data into a data structure\n",
    "    data = json.loads(json_data)\n",
    "\n",
    "# Now you can use the 'data' variable which contains the parsed JSON data\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8882286edd7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'original_corpus.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m         \u001b[0mfile1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'content' is not defined"
     ]
    }
   ],
   "source": [
    "with open('original_corpus.txt','w') as file1:\n",
    "        file1.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SpellChecker \n",
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "misspelled_words = spell.unknown(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('grfts', ['grafts', 'grits', 'gifts', 'grists', 'gravitas', 'graft'])])\n",
      "grfts\n",
      "grafts\n",
      "dict_items([('sapraemia', ['Semiramis', 'misapplier', 'supremacy', 'supremal'])])\n",
      "sapraemia\n",
      "Semiramis\n",
      "dict_items([('mnage', ['mange', 'menage', 'manage', 'nonage', 'nagged', 'nagger', 'Germana', 'munge', 'manège'])])\n",
      "mnage\n",
      "mange\n",
      "dict_items([('Loded', ['Oded', 'Roded', 'Coded', 'Boded', 'L oded', 'Doled', 'Lodes', 'Lode', 'Loaded', 'Lorded', 'Lodged', 'Laded', 'Loped', 'Lobed', 'Loved'])])\n",
      "Loded\n",
      "Oded\n",
      "dict_items([('Araham', ['Abraham', 'Graham', 'Ara ham', 'Ara-ham', 'Tamarah', 'Maharajah', 'Markham', 'Amara', 'Ibrahim', 'Arrowhead'])])\n",
      "Araham\n",
      "Abraham\n",
      "dict_items([('Razumovski', ['Phrasemaking', 'Radiophysics'])])\n",
      "Razumovski\n",
      "Phrasemaking\n",
      "dict_items([('evastated', ['devastated', 'devastate', 'devastator', 'devastating', 'devastation', 'estated', 'overstate', 'Hofstadter'])])\n",
      "evastated\n",
      "devastated\n",
      "dict_items([('onations', ['nations', 'donations', 'orations', 'ovations', 'o nations', 'nationals', 'intonation', 'donation', 'citations', 'unctions', 'insinuation'])])\n",
      "onations\n",
      "nations\n",
      "dict_items([('Neihardt', ['Reinhardt', 'Bernhardt', 'Reinhard', 'Gerhardt', 'Rinehart', 'Openhearted'])])\n",
      "Neihardt\n",
      "Reinhardt\n",
      "dict_items([('camlet', ['caplet', 'hamlet', 'Hamlet', 'cam let', 'cam-let', 'Camile', 'Camille', 'Capulet', 'armlet', 'Camelot', 'Carmelita'])])\n",
      "camlet\n",
      "caplet\n"
     ]
    }
   ],
   "source": [
    "for i in data:\n",
    "    # print(i)\n",
    "    print(i.items())\n",
    "    for key, value in i.items():\n",
    "        print(key)\n",
    "        print(value[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17062"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(misspelled_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1110379/1110379 [00:07<00:00, 146232.86it/s]\n"
     ]
    }
   ],
   "source": [
    "import enchant\n",
    "ench_d = enchant.Dict(\"en_US\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "misspelled =[]\n",
    "for word in tqdm(filtered_tokens):\n",
    "    if ench_d.check(word) == False:\n",
    "        misspelled.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# removing Duplicats \n",
    "misspelled_words=set(misspelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17069"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(misspelled_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17069/17069 [14:45<00:00, 19.27it/s]\n"
     ]
    }
   ],
   "source": [
    "file3 = open('enchant.txt', 'w')\n",
    "for word in tqdm(misspelled_words):\n",
    "    corrected_word = ench_d.suggest(word)\n",
    "    s=f\"{word}-->{corrected_word}\\n \"\n",
    "    file3.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('enchant.txt', 'r') as file:\n",
    "    enchant = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17010"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enchlist=enchant.split('\\n')\n",
    "len(enchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "str=\"\"\"['content', 'contemn', 'contempt', 'contrite', 'contuse', 'contested']\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'content'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str.split(\"'\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/gops/projects/woodfrog.tech/spellchecker/enchant_suggestion.txt', 'r') as file:\n",
    "    enchant = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_list=enchant.split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [00:00<00:01, 62.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tula tuna\n",
      "extremites extremities\n",
      "unresonable unreasonable\n",
      "lloy alloy\n",
      "tuffier stuffier\n",
      "grnular granular\n",
      "brasdor Labrador\n",
      "aprt part\n",
      "lichtenstein Lichtenstein\n",
      "novgorod Novgorod\n",
      "diretly tiredly\n",
      "dronushka anhydrous\n",
      "sribes scribes\n",
      "inequitble inequitable\n",
      "ouceur source\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [00:00<00:01, 67.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loth lot\n",
      "tolerble tolerable\n",
      "parma para\n",
      "vsilich silicosis\n",
      "exeutive executive\n",
      "vanka Valenka\n",
      "unresoning unreasoning\n",
      "infantwy infantry\n",
      "laret later\n",
      "memphis Memphis\n",
      "oklhoma Oklahoma\n",
      "fluis flews\n",
      "zkhar hark\n",
      "billroth billionth\n",
      "utomatic automatic\n",
      "galitsyn legality\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [00:00<00:00, 67.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trepak repack\n",
      "desslles leaderless\n",
      "potto potato\n",
      "shools shooks\n",
      "muscularis muscular is\n",
      "shrewness shrewdness\n",
      "hirless hitless\n",
      "silvanuses Silvanus\n",
      "permnganate peregrinate\n",
      "wostovs Rostov\n",
      "neutrlising neutralizing\n",
      "grsped graped\n",
      "srew drew\n",
      "reannexation re annexation\n",
      "lear lair\n",
      "epression repression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59/100 [00:00<00:00, 63.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oiffure coiffure\n",
      "possibilite possibility\n",
      "dorsum doors\n",
      "bolkonskis onionskins\n",
      "migrtion migration\n",
      "acteriology bacteriology\n",
      "juntions junctions\n",
      "smson Samson\n",
      "minerl miner\n",
      "listene listen\n",
      "plnter planter\n",
      "fford ford\n",
      "orgnic organic\n",
      "pvlograd Volgograd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [00:01<00:00, 65.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "putrefctive putrefactive\n",
      "menest meanest\n",
      "hemophilic hemophiliac\n",
      "aenomas melanomas\n",
      "terless waterless\n",
      "muh hum\n",
      "rchive chive\n",
      "enfants infants\n",
      "sholarship scholarship\n",
      "mrry merry\n",
      "svayka Svalbard\n",
      "vincent Vincent\n",
      "vitory victory\n",
      "provoatively provocatively\n",
      "ktie kite\n",
      "regultion regulation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 87/100 [00:01<00:00, 59.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oupled coupled\n",
      "lpse lose\n",
      "grves graves\n",
      "hunwed unwed\n",
      "pulmonry pulmonary\n",
      "slmon salmon\n",
      "meial meal\n",
      "tuscaroras Tuscarora\n",
      "oui our\n",
      "daptation adaptation\n",
      "cauterise cauterize\n",
      "zakharchenko Chernenko\n",
      "ompile compile\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 63.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shherbaty sherbet\n",
      "mchine chine\n",
      "omestic domestic\n",
      "aristoratic aristocratic\n",
      "mesure resume\n",
      "enlrgement enlargement\n",
      "streptoocci streptococci\n",
      "cllous callous\n",
      "efiling filing\n",
      "thetricals theatricals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12802237"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "file4 = open('corrected_corpus.txt', 'w')\n",
    "\n",
    "for elem in tqdm(enc_list[0:100]):\n",
    "    og_wrd=elem.split('-->')[0].strip()\n",
    "    try:\n",
    "        re_str=elem.split('-->')[1]\n",
    "        re_wrd=re_str.split(\"'\")[1].strip()\n",
    "        new_content=content.replace(og_wrd,re_wrd)\n",
    "        print(og_wrd,re_wrd)\n",
    "    except:\n",
    "        print(\"End of File\",elem)\n",
    "    # print(f\"og_wrd:{og_wrd} and replace with {re_wrd}\")\n",
    "file4.write(new_content)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:00<00:01,  5.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tula tuna\n",
      "extremites extremities\n",
      "unresonable unreasonable\n",
      "lloy alloy\n",
      "tuffier stuffier\n",
      "grnular granular\n",
      "brasdor Labrador\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 22.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aprt part\n",
      "lichtenstein Lichtenstein\n",
      "novgorod Novgorod\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for elem in tqdm(enc_list[0:10]):\n",
    "    og_wrd=elem.split('-->')[0].strip()\n",
    "    try:\n",
    "        re_str=elem.split('-->')[1]\n",
    "        re_wrd=re_str.split(\"'\")[1].strip()\n",
    "        new_content=content.replace(og_wrd,re_wrd)\n",
    "        print(og_wrd,re_wrd)\n",
    "    except:\n",
    "        print(\"End of File\",elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12802253"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file4.write(new_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " \"'\",\n",
       " 'c',\n",
       " 'o',\n",
       " 'n',\n",
       " 't',\n",
       " 'e',\n",
       " 'n',\n",
       " 't',\n",
       " \"'\",\n",
       " ',',\n",
       " ' ',\n",
       " \"'\",\n",
       " 'c',\n",
       " 'o',\n",
       " 'n',\n",
       " 't',\n",
       " 'e',\n",
       " 'm',\n",
       " 'n',\n",
       " \"'\",\n",
       " ',',\n",
       " ' ',\n",
       " \"'\",\n",
       " 'c',\n",
       " 'o',\n",
       " 'n',\n",
       " 't',\n",
       " 'e',\n",
       " 'm',\n",
       " 'p',\n",
       " 't',\n",
       " \"'\",\n",
       " ',',\n",
       " ' ',\n",
       " \"'\",\n",
       " 'c',\n",
       " 'o',\n",
       " 'n',\n",
       " 't',\n",
       " 'r',\n",
       " 'i',\n",
       " 't',\n",
       " 'e',\n",
       " \"'\",\n",
       " ',',\n",
       " ' ',\n",
       " \"'\",\n",
       " 'c',\n",
       " 'o',\n",
       " 'n',\n",
       " 't',\n",
       " 'u',\n",
       " 's',\n",
       " 'e',\n",
       " \"'\",\n",
       " ',',\n",
       " ' ',\n",
       " \"'\",\n",
       " 'c',\n",
       " 'o',\n",
       " 'n',\n",
       " 't',\n",
       " 'e',\n",
       " 's',\n",
       " 't',\n",
       " 'e',\n",
       " 'd',\n",
       " \"'\",\n",
       " ']']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=content[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Projet Gutenberg EBook of The Golden Bough (Third Edition, Vol. 8 of\\n2) by James George Frazer\\n\\n\\n\\nThis eBook is for the use of nyone anywhere at no cost and with almost no\\nrestritions whatsoever. '"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Projet Gutenberg EBook fffff The Golden Bough (Third Edition, Vol. 8 fffff\\n2) by James George Frazer\\n\\n\\n\\nThis eBook is for the use fffff nyone anywhere at no cost and with almost no\\nrestritions whatsoever. '"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e=f.replace('of',\"fffff\")\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Projet Gutenberg EBook of The Golden Bough (Third Edition, Vol. 8 of\\n2) by James George Frazer\\n\\n\\n\\nThis eBook is for the use of nyone anywhere at no cost and with almost no\\nrestritions whatsoever. '"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpellChecker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17062/17062 [17:56<00:00, 15.85it/s]\n"
     ]
    }
   ],
   "source": [
    "## SpellChecker \n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "\n",
    "file2 = open('spellchecker.txt', 'w')\n",
    "\n",
    "spell = SpellChecker()\n",
    "misspelled = spell.unknown(filtered_tokens)\n",
    "\n",
    "for word in tqdm(misspelled):\n",
    "    corrected_word = spell.correction(word)\n",
    "    s=f\"{word}-->{corrected_word} \\n \"\n",
    "    file2.write(s)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enchant Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4604/4604 [00:00<00:00, 82593.01it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# find those words that may be misspelled\n",
    "import enchant\n",
    "ench_d = enchant.Dict(\"en_US\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "misspelled =[]\n",
    "for i in tqdm(filtered_tokens):\n",
    "    if ench_d.check(word) == False:\n",
    "        misspelled.append(word)\n",
    "        \n",
    "file3 = open('enchant.txt', 'w')\n",
    "for word in tqdm(misspelled):\n",
    "    corrected_word = ench_d.suggest(word)\n",
    "    s=f\"{word}-->{corrected_word} \\n \"\n",
    "    file3.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "##update document \n",
    "file4 = open('enchant.txt', 'w')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
