{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" \n",
    "  This script will read the text corups(file location mentioned in filepath.txt file) \n",
    "  and create a dictonary with word frequency  . \n",
    "\"\"\"\n",
    "##---------------------------------------------------------------------\n",
    "# importing modules that we are going to need \n",
    "## ---------------------------------------------------------------------\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json \n",
    "import re\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('words') \n",
    "### ------------------------ Helper functions --------------------------\n",
    "def is_valid_word(word):\n",
    "    # Regular expression pattern to match valid words\n",
    "    pattern = r\"^[a-zA-Z0-9\\-']+$\"\n",
    "    return re.match(pattern, word) is not None\n",
    "## ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "## ---------------------------------------------------------------------\n",
    "##  Reading the Input file which we need to correct (file location is mentioned in filepath.txt)\n",
    "## ---------------------------------------------------------------------\n",
    "with open(\"filepath.txt\", 'r') as file:\n",
    "    file_path = file.read()\n",
    "try:    \n",
    "    ## Reading the File here \n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    ## Converting into tokens (words).NOTE: word_tokenize is more efficient that spliting with whitespaces \n",
    "    tokens = word_tokenize(content)\n",
    "    word_frequency = {}\n",
    "\n",
    "    for token in tokens:\n",
    "        if token==\"END-OF-CORPUS\":\n",
    "            break           \n",
    "        if is_valid_word(token):\n",
    "            if token not in word_frequency:\n",
    "                word_frequency[token] = 1\n",
    "            else:\n",
    "                word_frequency[token] = +1 \n",
    "    \n",
    "    #storing the word frequency dictonary as json file for future use \n",
    "    with open('word_frequency.json', 'w') as file2:\n",
    "        json.dump(word_frequency, file2)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"File '{file_path}' not found.\")\n",
    "except IOError:\n",
    "    print(f\"Error reading file '{file_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "import json \n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "def levenshtein_distance(s1, s2):\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "    distances = range(len(s1) + 1)\n",
    "    for index2, char2 in enumerate(s2):\n",
    "        new_distances = [index2 + 1]\n",
    "        for index1, char1 in enumerate(s1):\n",
    "            if char1 == char2:\n",
    "                new_distances.append(distances[index1])\n",
    "            else:\n",
    "                new_distances.append(1 + min((distances[index1], distances[index1 + 1], new_distances[-1])))\n",
    "        distances = new_distances\n",
    "    return distances[-1]\n",
    "\n",
    "## Suggestion based on levenshtein distance \n",
    "\n",
    "def suggest_replacement(word, suggestions_count=5):\n",
    "\n",
    "    word_list = words.words()\n",
    "    suggestions = []\n",
    "    for w in word_list:\n",
    "        if len(w) == len(word) and w[0].islower() and w.isalpha():\n",
    "            distance = levenshtein_distance(word, w)\n",
    "            suggestions.append((w, distance))\n",
    "    suggestions.sort(key=lambda x: x[1])\n",
    "\n",
    "    return [s[0] for s in suggestions[:suggestions_count]]\n",
    "## ---------------------------------------------------------------------\n",
    "##  Reading the wordfrequency  file generated in prev script\n",
    "## ---------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 272 Words there are 108 mis-spelled words \n"
     ]
    }
   ],
   "source": [
    "with open('word_frequency.json', 'r') as file2:\n",
    "    json_words=json.loads(file2.read())\n",
    "\n",
    "tokens = [token for token in json_words]\n",
    "english_words = set(words.words())\n",
    "\n",
    "## Check if the word is misspelled or not  (NOTE: this will include Nouns since this is part of english dictonary)\n",
    "misspelled_tokens = [token for token in tokens if token.lower() not in english_words]\n",
    "\n",
    "print(f\"Out of {len(tokens)} Words there are {len(misspelled_tokens)} mis-spelled words \")\n",
    "\n",
    "json_output=[]\n",
    "for word in tqdm(misspelled_list):\n",
    "    corrected_word = suggest_replacement(word)\n",
    "    json_output.append({word:corrected_word})\n",
    "\n",
    "with open('dictonary_of_suggestion.json', 'w') as file:\n",
    "        json.dump(json_output, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bollywood',\n",
       " 'Movies',\n",
       " 'Indian',\n",
       " 'has',\n",
       " 'audiences',\n",
       " 'worldwide',\n",
       " 'decades',\n",
       " 'foot-tapping',\n",
       " 'mesmerizing',\n",
       " 'sequences',\n",
       " 'movies',\n",
       " 'created',\n",
       " 'black-and-white',\n",
       " 'glitzy',\n",
       " 'productions',\n",
       " \"'s\",\n",
       " '1',\n",
       " '1940s',\n",
       " '1950s',\n",
       " 'including',\n",
       " 'classics',\n",
       " 'India',\n",
       " \"''\",\n",
       " 'Mughal-e-Azam',\n",
       " 'Sholay',\n",
       " 'actors',\n",
       " 'Kapoor',\n",
       " 'Dilip',\n",
       " 'Kumar',\n",
       " 'Nargis',\n",
       " 'harts',\n",
       " 'performances',\n",
       " '2',\n",
       " '1960s',\n",
       " '1970s',\n",
       " 'Deewar',\n",
       " 'Aradhana',\n",
       " 'Rajesh',\n",
       " 'Khanna',\n",
       " 'became',\n",
       " '3',\n",
       " 'Action-packed',\n",
       " '1980s',\n",
       " 'action-packed',\n",
       " 'blockbusters',\n",
       " 'Dhoom',\n",
       " 'Rambo',\n",
       " 'Kranti',\n",
       " 'Amitabh',\n",
       " 'Bachchan',\n",
       " 'emerged',\n",
       " '4',\n",
       " 'Tunes',\n",
       " 'directors',\n",
       " 'Burman',\n",
       " 'Laxmikant',\n",
       " 'Pyarelal',\n",
       " 'Rahman',\n",
       " 'melodies',\n",
       " 'etched',\n",
       " '5',\n",
       " 'Duos',\n",
       " 'on-screen',\n",
       " 'couples',\n",
       " 'Kajol',\n",
       " 'duos',\n",
       " 'minds',\n",
       " '6',\n",
       " 'Borders',\n",
       " 'Lagaan',\n",
       " 'Slumdog',\n",
       " 'introducing',\n",
       " '7',\n",
       " 'Choreographers',\n",
       " 'Saroj',\n",
       " 'Farah',\n",
       " 'cherished',\n",
       " 'fans',\n",
       " '8',\n",
       " '21st',\n",
       " 'Idiots',\n",
       " 'Bajrangi',\n",
       " 'Bhaijaan',\n",
       " 'Dangal',\n",
       " 'redefined',\n",
       " 'records',\n",
       " '9',\n",
       " 'Hollywood',\n",
       " 'collaborations',\n",
       " 'Bollywood-themed',\n",
       " 'parties',\n",
       " 'continues',\n",
       " '10',\n",
       " 'Misspelled',\n",
       " 'Words',\n",
       " 'misspelled',\n",
       " 'words',\n",
       " 'Dhom',\n",
       " 'spelled',\n",
       " 'Cranti',\n",
       " 'misspellings',\n",
       " 'titles',\n",
       " 'transcended',\n",
       " 'boundaries',\n",
       " 'stories',\n",
       " 'characters',\n",
       " 'evolves',\n",
       " 'moviegoers']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misspelled_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
