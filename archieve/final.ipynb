{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Read successul and found 1196734 Tokens/Words \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1196734/1196734 [00:09<00:00, 120955.15it/s]\n",
      "  0%|          | 3/17475 [00:00<11:56, 24.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 1196734 Words there are 193561 mis-spelled words and 17475 distinct mispelled words \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17475/17475 [15:16<00:00, 19.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All suggestions are loaded to 'enchant_suggestion.txt' file for Manual review \n"
     ]
    }
   ],
   "source": [
    "#import modules \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')  \n",
    "import enchant\n",
    "\n",
    "def checkspellerror(file_path):\n",
    "\n",
    "    try:\n",
    "\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "\n",
    "        #cleaning of text \n",
    "        tokens = word_tokenize(content)\n",
    "        \n",
    "\n",
    "        # Cleaning- optional \n",
    "        cleaned_tokens = [token for token in tokens if token.isalpha()]\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        \n",
    "        filtered_tokens = [token for token in cleaned_tokens if token not in stopwords]\n",
    "        # filtered_tokens=tokens \n",
    "        \n",
    "        print(f\"File Read successul and found {len(filtered_tokens)} Tokens/Words \")\n",
    "        \n",
    "        # Loading Enchant Dictionary \n",
    "\n",
    "        ench_d = enchant.Dict(\"en_US\")\n",
    "\n",
    "        from tqdm import tqdm\n",
    "        misspelled =[]\n",
    "        for word in tqdm(filtered_tokens):\n",
    "            if ench_d.check(word) == False:\n",
    "                misspelled.append(word)\n",
    "        misspelled_list=list(set(misspelled))\n",
    "        \n",
    "        print(f\"Out of {len(filtered_tokens)} Words there are {len(misspelled)} mis-spelled words and {len(misspelled_list)} distinct mispelled words \")\n",
    "\n",
    "        ## Finding Nearest Correct spelling 5 near words \n",
    "\n",
    "        file3 = open('enchant_suggestion.txt', 'w')\n",
    "\n",
    "        for word in tqdm(misspelled_list):\n",
    "            corrected_word = ench_d.suggest(word)\n",
    "            s=f\"{word}-->{corrected_word}\\n \"\n",
    "            file3.write(s)\n",
    "        print(\"All suggestions are loaded to 'enchant_suggestion.txt' file for Manual review \")\n",
    "\n",
    "        with open('original_corpus.txt','w') as file1:\n",
    "             file1.write(content)\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{file_path}' not found.\")\n",
    "    except IOError:\n",
    "        print(f\"Error reading file '{file_path}'.\")\n",
    "\n",
    "file_path='/home/gops/projects/woodfrog.tech/spellchecker/data/corpus.txt'\n",
    "checkspellerror(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17475/17475 [1:33:25<00:00,  3.12it/s]\n"
     ]
    },
    {
     "ename": "UnsupportedOperation",
     "evalue": "not writable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-19828d43a990>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'corrected_corpus.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mfile2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnsupportedOperation\u001b[0m: not writable"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import re \n",
    "\n",
    "op_file = open('corrected_corpus.txt', 'w')\n",
    "\n",
    "with open('original_corpus.txt', 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "with open('enchant_suggestion.json', 'r') as file:\n",
    "    json_data = file.read()\n",
    "    data = json.loads(json_data)\n",
    "    \n",
    "new_content=content\n",
    "\n",
    "\n",
    "for elem in tqdm(data):\n",
    "    for key, value in elem.items():\n",
    "        rplc_w=key\n",
    "        crct_w=value[0]\n",
    "        pattern = r'\\b{}\\b'.format(re.escape(rplc_w))\n",
    "        new_content = re.sub(pattern, crct_w, new_content)\n",
    "\n",
    "with open('corrected_corpus.txt', 'w') as file2:\n",
    "        file2.write(new_content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(new_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corrected_corpus.txt', 'w') as file2:\n",
    "        file2.write(new_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='/home/gops/projects/woodfrog.tech/spellchecker/data/corpus.txt'\n",
    "try:\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "except FileNotFoundError:\n",
    "    print(f\"File '{file_path}' not found.\")\n",
    "except IOError:\n",
    "    print(f\"Error reading file '{file_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gops/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/gops/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1110379"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokens = word_tokenize(content)\n",
    "\n",
    "# Cleaning\n",
    "cleaned_tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "#\n",
    "filtered_tokens = [token for token in cleaned_tokens if token not in stopwords]\n",
    "\n",
    "# Display the extracted words\n",
    "len(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SpellChecker \n",
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "misspelled_words = spell.unknown(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17062"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(misspelled_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1110379/1110379 [00:07<00:00, 146232.86it/s]\n"
     ]
    }
   ],
   "source": [
    "import enchant\n",
    "ench_d = enchant.Dict(\"en_US\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "misspelled =[]\n",
    "for word in tqdm(filtered_tokens):\n",
    "    if ench_d.check(word) == False:\n",
    "        misspelled.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# removing Duplicats \n",
    "misspelled_words=set(misspelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17069"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(misspelled_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17069/17069 [14:45<00:00, 19.27it/s]\n"
     ]
    }
   ],
   "source": [
    "file3 = open('enchant.txt', 'w')\n",
    "for word in tqdm(misspelled_words):\n",
    "    corrected_word = ench_d.suggest(word)\n",
    "    s=f\"{word}-->{corrected_word}\\n \"\n",
    "    file3.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/gops/projects/woodfrog.tech/python spell check final/filepath.txt\", 'r') as file:\n",
    "    path = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/gops/projects/woodfrog.tech/python spell check final/corpus.txt'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path) as file:\n",
    "    corpus=file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Projet Gutenberg EBook of The Golden Bough (Third Edition, Vol. 8 of\\n2) by James George Frazer\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17010"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enchlist=enchant.split('\\n')\n",
    "len(enchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "__enter__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-046902bdc122>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/gops/projects/woodfrog.tech/python spell check final/corpus.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: __enter__"
     ]
    }
   ],
   "source": [
    "with (\"/home/gops/projects/woodfrog.tech/python spell check final/corpus.txt\",'r') as file:\n",
    "    filepath=file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpellChecker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17062/17062 [17:56<00:00, 15.85it/s]\n"
     ]
    }
   ],
   "source": [
    "## SpellChecker \n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "\n",
    "file2 = open('spellchecker.txt', 'w')\n",
    "\n",
    "spell = SpellChecker()\n",
    "misspelled = spell.unknown(filtered_tokens)\n",
    "\n",
    "for word in tqdm(misspelled):\n",
    "    corrected_word = spell.correction(word)\n",
    "    s=f\"{word}-->{corrected_word} \\n \"\n",
    "    file2.write(s)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enchant Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4604/4604 [00:00<00:00, 82593.01it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# find those words that may be misspelled\n",
    "import enchant\n",
    "ench_d = enchant.Dict(\"en_US\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "misspelled =[]\n",
    "for i in tqdm(filtered_tokens):\n",
    "    if ench_d.check(word) == False:\n",
    "        misspelled.append(word)\n",
    "        \n",
    "file3 = open('enchant.txt', 'w')\n",
    "for word in tqdm(misspelled):\n",
    "    corrected_word = ench_d.suggest(word)\n",
    "    s=f\"{word}-->{corrected_word} \\n \"\n",
    "    file3.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "##update document \n",
    "file4 = open('enchant.txt', 'w')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
