{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Read successul and found 1196734 Tokens/Words \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1196734/1196734 [00:09<00:00, 120955.15it/s]\n",
      "  0%|          | 3/17475 [00:00<11:56, 24.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 1196734 Words there are 193561 mis-spelled words and 17475 distinct mispelled words \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17475/17475 [15:16<00:00, 19.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All suggestions are loaded to 'enchant_suggestion.txt' file for Manual review \n"
     ]
    }
   ],
   "source": [
    "#import modules \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')  \n",
    "import enchant\n",
    "\n",
    "def checkspellerror(file_path):\n",
    "\n",
    "    try:\n",
    "\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "\n",
    "        #cleaning of text \n",
    "        tokens = word_tokenize(content)\n",
    "        \n",
    "\n",
    "        # Cleaning- optional \n",
    "        cleaned_tokens = [token for token in tokens if token.isalpha()]\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        \n",
    "        filtered_tokens = [token for token in cleaned_tokens if token not in stopwords]\n",
    "        # filtered_tokens=tokens \n",
    "        \n",
    "        print(f\"File Read successul and found {len(filtered_tokens)} Tokens/Words \")\n",
    "        \n",
    "        # Loading Enchant Dictionary \n",
    "\n",
    "        ench_d = enchant.Dict(\"en_US\")\n",
    "\n",
    "        from tqdm import tqdm\n",
    "        misspelled =[]\n",
    "        for word in tqdm(filtered_tokens):\n",
    "            if ench_d.check(word) == False:\n",
    "                misspelled.append(word)\n",
    "        misspelled_list=list(set(misspelled))\n",
    "        \n",
    "        print(f\"Out of {len(filtered_tokens)} Words there are {len(misspelled)} mis-spelled words and {len(misspelled_list)} distinct mispelled words \")\n",
    "\n",
    "        ## Finding Nearest Correct spelling 5 near words \n",
    "\n",
    "        file3 = open('enchant_suggestion.txt', 'w')\n",
    "\n",
    "        for word in tqdm(misspelled_list):\n",
    "            corrected_word = ench_d.suggest(word)\n",
    "            s=f\"{word}-->{corrected_word}\\n \"\n",
    "            file3.write(s)\n",
    "        print(\"All suggestions are loaded to 'enchant_suggestion.txt' file for Manual review \")\n",
    "\n",
    "        with open('original_corpus.txt','w') as file1:\n",
    "             file1.write(content)\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{file_path}' not found.\")\n",
    "    except IOError:\n",
    "        print(f\"Error reading file '{file_path}'.\")\n",
    "\n",
    "file_path='/home/gops/projects/woodfrog.tech/spellchecker/data/corpus.txt'\n",
    "checkspellerror(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17475/17475 [1:33:25<00:00,  3.12it/s]\n"
     ]
    },
    {
     "ename": "UnsupportedOperation",
     "evalue": "not writable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-19828d43a990>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'corrected_corpus.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mfile2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnsupportedOperation\u001b[0m: not writable"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import re \n",
    "\n",
    "op_file = open('corrected_corpus.txt', 'w')\n",
    "\n",
    "with open('original_corpus.txt', 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "with open('enchant_suggestion.json', 'r') as file:\n",
    "    json_data = file.read()\n",
    "    data = json.loads(json_data)\n",
    "    \n",
    "new_content=content\n",
    "\n",
    "\n",
    "for elem in tqdm(data):\n",
    "    for key, value in elem.items():\n",
    "        rplc_w=key\n",
    "        crct_w=value[0]\n",
    "        pattern = r'\\b{}\\b'.format(re.escape(rplc_w))\n",
    "        new_content = re.sub(pattern, crct_w, new_content)\n",
    "\n",
    "with open('corrected_corpus.txt', 'w') as file2:\n",
    "        file2.write(new_content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import json\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')  \n",
    "import enchant\n",
    "file_path='/home/gops/projects/woodfrog.tech/spellchecker/archieve/data/sampletext.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "def is_valid_word(word):\n",
    "    # Regular expression pattern to match valid words\n",
    "    pattern = r\"^[a-zA-Z0-9\\-']+$\"\n",
    "    return re.match(pattern, word) is not None\n",
    "\n",
    "\n",
    "#cleaning of text \n",
    "tokens = word_tokenize(content)\n",
    "word_frequency = {}\n",
    "cleaned_tokens=[]\n",
    "# Cleaning- optional \n",
    "# cleaned_tokens = [token for token in tokens if token.isalpha()] # keeping only text with all aplhabetic words\n",
    "for token in tokens:\n",
    "    if token not in word_frequency:\n",
    "        word_frequency[token] = 1\n",
    "    else:\n",
    "        word_frequency[token] = +1 \n",
    "    if token==\"END-OF-CORPUS\":\n",
    "        break           \n",
    "    if is_valid_word(token):\n",
    "        cleaned_tokens.append(token)\n",
    "\n",
    "\n",
    "with open('word_frequency.json', 'w') as file2:\n",
    "    json.dump(word_frequency, file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Artificial': 1,\n",
       " 'Inteligence': 1,\n",
       " '(': 1,\n",
       " 'AI': 1,\n",
       " ')': 1,\n",
       " 'is': 1,\n",
       " 'revolutinary': 1,\n",
       " 'and': 1,\n",
       " 'has': 1,\n",
       " 'a': 1,\n",
       " 'pwerful': 1,\n",
       " 'impact': 1,\n",
       " 'on': 1,\n",
       " 'our': 1,\n",
       " 'society': 1,\n",
       " '.': 1,\n",
       " 'It': 1,\n",
       " \"'s\": 1,\n",
       " 'transforming': 1,\n",
       " 'diverse': 1,\n",
       " 'industries': 1,\n",
       " ',': 1,\n",
       " 'includding': 1,\n",
       " 'healthcare': 1,\n",
       " 'finance': 1,\n",
       " 'tranportation': 1,\n",
       " 'algorythms': 1,\n",
       " 'can': 1,\n",
       " 'analyze': 1,\n",
       " 'vasts': 1,\n",
       " 'amounts': 1,\n",
       " 'of': 1,\n",
       " 'data': 1,\n",
       " 'unearthing': 1,\n",
       " 'valuable': 1,\n",
       " 'insghts': 1,\n",
       " 'making': 1,\n",
       " 'predctions': 1,\n",
       " 'Machne': 1,\n",
       " 'learnng': 1,\n",
       " 'keey': 1,\n",
       " 'component': 1,\n",
       " 'helpng': 1,\n",
       " 'computers': 1,\n",
       " 'to': 1,\n",
       " 'lern': 1,\n",
       " 'adapt': 1,\n",
       " 'basd': 1,\n",
       " 'paterns': 1,\n",
       " 'enhanses': 1,\n",
       " 'automtion': 1,\n",
       " 'efficieny': 1,\n",
       " 'prodcitivity': 1,\n",
       " 'The': 1,\n",
       " 'develpment': 1,\n",
       " 'requres': 1,\n",
       " 'skiled': 1,\n",
       " 'proffesionals': 1,\n",
       " 'who': 1,\n",
       " 'undrstand': 1,\n",
       " 'algorithsm': 1,\n",
       " 'create': 1,\n",
       " 'sophisticaed': 1,\n",
       " 'models': 1,\n",
       " 'Ths': 1,\n",
       " 'field': 1,\n",
       " 'contnues': 1,\n",
       " 'evolv': 1,\n",
       " 'rapidly': 1,\n",
       " 'opening': 1,\n",
       " 'new': 1,\n",
       " 'horizons': 1,\n",
       " 'opportunitis': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(new_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corrected_corpus.txt', 'w') as file2:\n",
    "        file2.write(new_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='/home/gops/projects/woodfrog.tech/spellchecker/data/corpus.txt'\n",
    "try:\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "except FileNotFoundError:\n",
    "    print(f\"File '{file_path}' not found.\")\n",
    "except IOError:\n",
    "    print(f\"Error reading file '{file_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gops/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/gops/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1110379"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokens = word_tokenize(content)\n",
    "\n",
    "# Cleaning\n",
    "cleaned_tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "#\n",
    "filtered_tokens = [token for token in cleaned_tokens if token not in stopwords]\n",
    "\n",
    "# Display the extracted words\n",
    "len(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SpellChecker \n",
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "misspelled_words = spell.unknown(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17062"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(misspelled_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1110379/1110379 [00:07<00:00, 146232.86it/s]\n"
     ]
    }
   ],
   "source": [
    "import enchant\n",
    "ench_d = enchant.Dict(\"en_US\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "misspelled =[]\n",
    "for word in tqdm(filtered_tokens):\n",
    "    if ench_d.check(word) == False:\n",
    "        misspelled.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# removing Duplicats \n",
    "misspelled_words=set(misspelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17069"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(misspelled_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17069/17069 [14:45<00:00, 19.27it/s]\n"
     ]
    }
   ],
   "source": [
    "file3 = open('enchant.txt', 'w')\n",
    "for word in tqdm(misspelled_words):\n",
    "    corrected_word = ench_d.suggest(word)\n",
    "    s=f\"{word}-->{corrected_word}\\n \"\n",
    "    file3.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/gops/projects/woodfrog.tech/python spell check final/filepath.txt\", 'r') as file:\n",
    "    path = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/gops/projects/woodfrog.tech/python spell check final/corpus.txt'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path) as file:\n",
    "    corpus=file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Projet Gutenberg EBook of The Golden Bough (Third Edition, Vol. 8 of\\n2) by James George Frazer\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17010"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enchlist=enchant.split('\\n')\n",
    "len(enchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "__enter__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-046902bdc122>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/gops/projects/woodfrog.tech/python spell check final/corpus.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: __enter__"
     ]
    }
   ],
   "source": [
    "with (\"/home/gops/projects/woodfrog.tech/python spell check final/corpus.txt\",'r') as file:\n",
    "    filepath=file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/gops/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misspelled word: appel\n",
      "Suggestions based on frequency:\n",
      "['appet', 'agnel', 'akpek']\n",
      "Suggestions based on edit distance:\n",
      "['appet', 'agnel', 'akpek']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "# Install and download the 'words' corpus from NLTK\n",
    "nltk.download('words')\n",
    "\n",
    "# Function to calculate the Levenshtein distance\n",
    "def levenshtein_distance(s1, s2):\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "\n",
    "    distances = range(len(s1) + 1)\n",
    "    for index2, char2 in enumerate(s2):\n",
    "        new_distances = [index2 + 1]\n",
    "        for index1, char1 in enumerate(s1):\n",
    "            if char1 == char2:\n",
    "                new_distances.append(distances[index1])\n",
    "            else:\n",
    "                new_distances.append(1 + min((distances[index1], distances[index1 + 1], new_distances[-1])))\n",
    "        distances = new_distances\n",
    "    return distances[-1]\n",
    "\n",
    "# Function to suggest replacements based on word frequency\n",
    "def suggest_replacement_frequency(word, suggestions_count=3):\n",
    "    word_list = words.words()\n",
    "    suggestions = []\n",
    "    for w in word_list:\n",
    "        if len(w) == len(word) and w[0].islower() and w.isalpha():\n",
    "            distance = levenshtein_distance(word, w)\n",
    "            suggestions.append((w, distance))\n",
    "    suggestions.sort(key=lambda x: x[1])\n",
    "    return [s[0] for s in suggestions[:suggestions_count]]\n",
    "\n",
    "# Function to suggest replacements based on edit distance\n",
    "def suggest_replacement_edit_distance(word, suggestions_count=3):\n",
    "    word_list = words.words()\n",
    "    suggestions = []\n",
    "    for w in word_list:\n",
    "        if len(w) == len(word) and w[0].islower() and w.isalpha():\n",
    "            distance = levenshtein_distance(word, w)\n",
    "            suggestions.append((w, distance))\n",
    "    suggestions.sort(key=lambda x: x[1])\n",
    "    return [s[0] for s in suggestions[:suggestions_count]]\n",
    "\n",
    "# Example usage\n",
    "misspelled_word = \"appel\"\n",
    "print(f\"Misspelled word: {misspelled_word}\")\n",
    "\n",
    "# Suggest replacements based on word frequency\n",
    "frequency_suggestions = suggest_replacement_frequency(misspelled_word)\n",
    "print(\"Suggestions based on frequency:\")\n",
    "print(frequency_suggestions)\n",
    "\n",
    "# Suggest replacements based on edit distance\n",
    "edit_distance_suggestions = suggest_replacement_edit_distance(misspelled_word)\n",
    "print(\"Suggestions based on edit distance:\")\n",
    "print(edit_distance_suggestions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpellChecker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17062/17062 [17:56<00:00, 15.85it/s]\n"
     ]
    }
   ],
   "source": [
    "## SpellChecker \n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "\n",
    "file2 = open('spellchecker.txt', 'w')\n",
    "\n",
    "spell = SpellChecker()\n",
    "misspelled = spell.unknown(filtered_tokens)\n",
    "\n",
    "for word in tqdm(misspelled):\n",
    "    corrected_word = spell.correction(word)\n",
    "    s=f\"{word}-->{corrected_word} \\n \"\n",
    "    file2.write(s)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enchant Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4604/4604 [00:00<00:00, 82593.01it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# find those words that may be misspelled\n",
    "import enchant\n",
    "ench_d = enchant.Dict(\"en_US\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "misspelled =[]\n",
    "for i in tqdm(filtered_tokens):\n",
    "    if ench_d.check(word) == False:\n",
    "        misspelled.append(word)\n",
    "        \n",
    "file3 = open('enchant.txt', 'w')\n",
    "for word in tqdm(misspelled):\n",
    "    corrected_word = ench_d.suggest(word)\n",
    "    s=f\"{word}-->{corrected_word} \\n \"\n",
    "    file3.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "##update document \n",
    "file4 = open('enchant.txt', 'w')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
